{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "yDtkhWqDLPAq"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3GLuRaTlLPAv"
   },
   "source": [
    "Boilerplate for graph visualization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "0Vnl3jP_LPAx"
   },
   "outputs": [],
   "source": [
    "# This is for graph visualization.\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5r42SomULPA2"
   },
   "source": [
    "Load the data\n",
    "---\n",
    "Run 00_download_data.ipynb if you haven't already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "OzoRsG-5LPA3",
    "outputId": "ef95b4a0-cef9-4671-fa63-14c2791d855c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes:  (1000, 784) (1000, 10) (9000, 784) (9000, 10)\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data/'\n",
    "data_filename = os.path.join(DATA_DIR, \"zoo.npz\")\n",
    "data = np.load(open(data_filename))\n",
    "\n",
    "train_data = data['arr_0']\n",
    "train_labels = data['arr_1']\n",
    "test_data = data['arr_2']\n",
    "test_labels = data['arr_3']\n",
    "del data\n",
    "print(\"Data shapes: \", test_data.shape, test_labels.shape, train_data.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simple classifier with low-level TF Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 8292,
     "status": "error",
     "timestamp": 1497369977531,
     "user": {
      "displayName": "Christian Buck",
      "photoUrl": "//lh5.googleusercontent.com/-i04j1OZ3aUs/AAAAAAAAAAI/AAAAAAAAABc/Dx1Bhh2XWUE/s50-c-k-no/photo.jpg",
      "userId": "108154180342320802225"
     },
     "user_tz": -120
    },
    "id": "odSuMrBULPA9",
    "outputId": "6182b273-3fb2-486e-dbf8-7147c7de99f0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.294353982511&quot;).pbtxt = 'node {\\n  name: &quot;data&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;labels&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated_normal/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\020\\\\003\\\\000\\\\000\\\\200\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated_normal/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated_normal/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0357142873108\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated_normal/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;truncated_normal/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated_normal/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;truncated_normal/TruncatedNormal&quot;\\n  input: &quot;truncated_normal/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated_normal&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;truncated_normal/mul&quot;\\n  input: &quot;truncated_normal/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;weights_1&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 784\\n        }\\n        dim {\\n          size: 128\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;weights_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;weights_1&quot;\\n  input: &quot;truncated_normal&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@weights_1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;weights_1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;weights_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@weights_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated_normal_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\200\\\\000\\\\000\\\\000\\\\n\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated_normal_1/mean&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated_normal_1/stddev&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0883883461356\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated_normal_1/TruncatedNormal&quot;\\n  op: &quot;TruncatedNormal&quot;\\n  input: &quot;truncated_normal_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 0\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated_normal_1/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;truncated_normal_1/TruncatedNormal&quot;\\n  input: &quot;truncated_normal_1/stddev&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truncated_normal_1&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;truncated_normal_1/mul&quot;\\n  input: &quot;truncated_normal_1/mean&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;weights_2&quot;\\n  op: &quot;Variable&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 128\\n        }\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;weights_2/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;weights_2&quot;\\n  input: &quot;truncated_normal_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@weights_2&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;weights_2/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;weights_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@weights_2&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;data&quot;\\n  input: &quot;weights_1/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;MatMul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;Relu&quot;\\n  input: &quot;weights_2/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Tanh&quot;\\n  op: &quot;Tanh&quot;\\n  input: &quot;MatMul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;labels&quot;\\n  input: &quot;Tanh&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/L2Loss&quot;\\n  op: &quot;L2Loss&quot;\\n  input: &quot;loss/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.294353982511&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "input_dimension = train_data.shape[1]   # 784 = 28*28 pixels\n",
    "output_dimension = train_labels.shape[1]  # 10 classes\n",
    "\n",
    "batch_size = 32\n",
    "hidden1_units = 128\n",
    "\n",
    "data_batch =  tf.placeholder(\"float\", shape=[None, input_dimension], name=\"data\")\n",
    "label_batch = tf.placeholder(\"float\", shape=[None, output_dimension], name=\"labels\")\n",
    "\n",
    "weights_1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [input_dimension, hidden1_units], \n",
    "        stddev=1.0 / np.sqrt(float(input_dimension))),\n",
    "    name='weights_1')\n",
    "\n",
    "# Task: Add Bias to first layer\n",
    "# Task: Use Cross-Entropy instead of Squared Loss\n",
    "\n",
    "weights_2 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [hidden1_units, output_dimension], \n",
    "        stddev=1.0 / np.sqrt(float(hidden1_units))),\n",
    "    name='weights_2')\n",
    "\n",
    "wx_b = tf.matmul(data_batch, weights_1)\n",
    "hidden_activations = tf.nn.relu(wx_b)\n",
    "output_activations = tf.nn.tanh(tf.matmul(hidden_activations, weights_2))\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.nn.l2_loss(label_batch - output_activations)\n",
    "\n",
    "show_graph(tf.get_default_graph().as_graph_def())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BnAyjAXnLPBB"
   },
   "source": [
    "We can run this graph by feeding in batches of examples using a feed_dict. The keys of the feed_dict are placeholders we've defined previously.\n",
    "The first argument of session.run is the tensor that we're computing. Only parts of the graph required to produce this value will be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "GDn0ixriLPBC",
    "outputId": "43d24a35-8774-47bc-ff45-9ee2ae36125e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 100: 27.2944602966\n",
      "Loss at iteration 200: 26.6942863464\n",
      "Loss at iteration 300: 27.1933517456\n",
      "Loss at iteration 400: 28.1028251648\n",
      "Loss at iteration 500: 27.8015823364\n",
      "Loss at iteration 600: 28.2706413269\n",
      "Loss at iteration 700: 28.6937580109\n",
      "Loss at iteration 800: 29.1431274414\n",
      "Loss at iteration 900: 27.5183849335\n",
      "Loss at iteration 1000: 26.8373718262\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "  \n",
    "    # We'll pick slices from a random permutation to randomize batches during training\n",
    "    random_indices = np.random.permutation(train_data.shape[0])\n",
    "    for i in range(1000):\n",
    "        batch_start_idx = (i % (train_data.shape[0] // batch_size)) * batch_size\n",
    "        batch_indices = random_indices[batch_start_idx:batch_start_idx + batch_size]\n",
    "        batch_loss = sess.run(\n",
    "            loss, \n",
    "            feed_dict = {\n",
    "                data_batch : train_data[batch_indices,:],\n",
    "                label_batch : train_labels[batch_indices,:]\n",
    "            })\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"Loss at iteration {}: {}\".format(i+1, batch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Je87CgiLPBF"
   },
   "source": [
    "No learning yet but we get the losses per batch.\n",
    "We need to add an optimizer to the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "IYoyWqrgLPBG",
    "outputId": "16393ac8-48b4-4bd3-8627-458ccbba7be8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch-Loss at iteration 0: 30.1927909851\n",
      "Accuracy on test: 13.7%\n",
      "Batch-Loss at iteration 200: 11.2681150436\n",
      "Accuracy on test: 45.1%\n",
      "Batch-Loss at iteration 400: 11.6937246323\n",
      "Accuracy on test: 50.0%\n",
      "Batch-Loss at iteration 600: 11.5805540085\n",
      "Accuracy on test: 52.5%\n",
      "Batch-Loss at iteration 800: 9.28488349915\n",
      "Accuracy on test: 52.4%\n",
      "Batch-Loss at iteration 1000: 10.8607110977\n",
      "Accuracy on test: 53.7%\n",
      "Batch-Loss at iteration 1200: 7.73901033401\n",
      "Accuracy on test: 54.2%\n",
      "Batch-Loss at iteration 1400: 5.72501659393\n",
      "Accuracy on test: 53.9%\n",
      "Batch-Loss at iteration 1600: 8.97097301483\n",
      "Accuracy on test: 54.1%\n",
      "Batch-Loss at iteration 1800: 9.96781253815\n",
      "Accuracy on test: 55.7%\n",
      "Batch-Loss at iteration 2000: 8.90931892395\n",
      "Accuracy on test: 56.0%\n",
      "Batch-Loss at iteration 2200: 8.42798995972\n",
      "Accuracy on test: 54.6%\n",
      "Batch-Loss at iteration 2400: 9.61281967163\n",
      "Accuracy on test: 55.0%\n",
      "Batch-Loss at iteration 2600: 7.13080787659\n",
      "Accuracy on test: 55.8%\n",
      "Batch-Loss at iteration 2800: 9.17970466614\n",
      "Accuracy on test: 57.3%\n",
      "Batch-Loss at iteration 2811: 5.55652618408\n",
      "Accuracy on test: 55.6%\n"
     ]
    }
   ],
   "source": [
    "# Task: Replace GradientDescentOptimizer with AdagradOptimizer and a 0.1 learning rate.\n",
    "learning_rate = 0.005\n",
    "updates = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    random_indices = np.random.permutation(train_data.shape[0])\n",
    "    n_epochs = 10  # how often do to go through the training data\n",
    "    max_steps = train_data.shape[0]*n_epochs // batch_size\n",
    "    for i in range(max_steps):\n",
    "        batch_start_idx = (i % (train_data.shape[0] // batch_size)) * batch_size\n",
    "        batch_indices = random_indices[batch_start_idx:batch_start_idx+batch_size]\n",
    "        batch_loss, _ = sess.run(\n",
    "            [loss, updates], \n",
    "            feed_dict = {\n",
    "                data_batch : train_data[batch_indices,:],\n",
    "                label_batch : train_labels[batch_indices,:]\n",
    "            })\n",
    "\n",
    "        if i % 200 == 0 or i == max_steps - 1:\n",
    "            random_indices = np.random.permutation(train_data.shape[0])\n",
    "            print(\"Batch-Loss at iteration {}: {}\".format(i, batch_loss))\n",
    "\n",
    "            test_predictions = sess.run(\n",
    "                output_activations, \n",
    "                feed_dict = {\n",
    "                    data_batch : test_data,\n",
    "                    label_batch : test_labels\n",
    "                })\n",
    "            wins = np.argmax(test_predictions, axis=1) == np.argmax(test_labels, axis=1)\n",
    "            print(\"Accuracy on test: {}%\".format(100*np.mean(wins)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VaEx4NK_LPBJ"
   },
   "source": [
    "Loss going down, Accuracy going up! \\o/\n",
    "\n",
    "Notice how batch loss differs between batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AlR_4etuLPBK"
   },
   "source": [
    "# Model wrapped in a custom estimator\n",
    "In TensorFlow, we can make it easier to experiment with different models when we separately define a model_fn and an input_fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {},
      {},
      {},
      {},
      {}
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "2_sBrEKhLPBL",
    "outputId": "fc5468ca-7e24-4a65-b6c5-aaf028f8a627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/yn/fxh0wpgd1wq7w83m5fh2mqkw0000gp/T/tmp2DtA2U\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'save_summary_steps': 100, '_num_ps_replicas': 0, '_task_type': None, '_environment': 'local', '_is_chief': True, 'save_checkpoints_secs': 600, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11625c290>, 'tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_task_id': 0, 'tf_random_seed': None, 'keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', 'save_checkpoints_steps': None, '_master': '', 'keep_checkpoint_max': 5}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x115f39f50>) includes params argument, but params are not passed to Estimator.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'ModelFnOps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-71db946a0b14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-71db946a0b14>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, input_fn)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mmax_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_epochs\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         print(model.evaluate(input_fn=lambda: input_fn(test_data, test_labels),\n\u001b[1;32m     78\u001b[0m                                                  steps=150))\n",
      "\u001b[0;32m/Users/azaidi/anaconda/envs/cs224n/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0m_call_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             func.__module__, arg_name, date, instructions)\n\u001b[0;32m--> 191\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\n\u001b[1;32m    193\u001b[0m         func.__doc__, date, instructions)\n",
      "\u001b[0;32m/Users/azaidi/anaconda/envs/cs224n/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[1;32m    353\u001b[0m                              \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                              \u001b[0mmonitors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmonitors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                              max_steps=max_steps)\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/azaidi/anaconda/envs/cs224n/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)\u001b[0m\n\u001b[1;32m    697\u001b[0m       \u001b[0;31m# cases, but will soon be deleted after the subclasses are updated.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m       \u001b[0;31m# TODO(b/32664904): Update subclasses and delete the else-statement.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m       \u001b[0mtrain_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelFnOps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Default signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/azaidi/anaconda/envs/cs224n/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc\u001b[0m in \u001b[0;36m_get_train_ops\u001b[0;34m(self, features, labels)\u001b[0m\n\u001b[1;32m   1050\u001b[0m       \u001b[0;34m`\u001b[0m\u001b[0mModelFnOps\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m     \"\"\"\n\u001b[0;32m-> 1052\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_eval_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/azaidi/anaconda/envs/cs224n/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode)\u001b[0m\n\u001b[1;32m   1017\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'params'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_fn_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m         model_fn_results = self._model_fn(features, labels, mode=mode,\n\u001b[0;32m-> 1019\u001b[0;31m                                           params=self.params)\n\u001b[0m\u001b[1;32m   1020\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-71db946a0b14>\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, targets, mode, params)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# 5. Return predictions/loss/train_op/eval_metric_ops in ModelFnOps object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     return tf.contrib.learn.ModelFnOps(\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'ModelFnOps'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Model parameters.\n",
    "batch_size = 32\n",
    "hidden1_units = 128\n",
    "learning_rate = 0.005\n",
    "input_dimension = train_data.shape[1]   # 784 = 28*28 pixels\n",
    "output_dimension = train_labels.shape[1]  # 6 classes\n",
    "n_epochs = 10  # how often do to go through the training data\n",
    "\n",
    "\n",
    "def input_fn(data, labels):\n",
    "    input_images = tf.constant(data, shape=data.shape, verify_shape=True, dtype=tf.float32)\n",
    "    input_labels = tf.constant(labels, shape=labels.shape, verify_shape=True, dtype=tf.float32)\n",
    "    image, label = tf.train.slice_input_producer(\n",
    "        [input_images, input_labels],\n",
    "        num_epochs=n_epochs)\n",
    "    dataset_dict = dict(images=image, labels=label)\n",
    "    batch_dict = tf.train.batch(\n",
    "        dataset_dict, batch_size, allow_smaller_final_batch=True)\n",
    "    batch_labels = batch_dict.pop('labels')\n",
    "    return batch_dict, batch_labels\n",
    "\n",
    "\n",
    "def model_fn(features, targets, mode, params):\n",
    "    # 1. Configure the model via TensorFlow operations (same as above)\n",
    "    weights_1 = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                    [input_dimension, hidden1_units],\n",
    "                    stddev=1.0 / np.sqrt(float(input_dimension))))\n",
    "    weights_2 = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                    [hidden1_units, output_dimension],\n",
    "                    stddev=1.0 / np.sqrt(float(hidden1_units))))\n",
    "    hidden_activations = tf.nn.relu(tf.matmul(features['images'], weights_1))\n",
    "    output_activations = tf.matmul(hidden_activations, weights_2)\n",
    "    \n",
    "    # 2. Define the loss function for training/evaluation\n",
    "    loss = tf.reduce_mean(tf.nn.l2_loss(targets - output_activations))\n",
    "    \n",
    "    # 3. Define the training operation/optimizer\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            learning_rate=learning_rate,\n",
    "            optimizer=\"SGD\")\n",
    "    \n",
    "    # 4. Generate predictions\n",
    "    predictions_dict = {\n",
    "        \"classes\":       tf.argmax(input=output_activations, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(output_activations, name=\"softmax_tensor\"), \n",
    "        \"logits\":        output_activations,\n",
    "    }\n",
    "    \n",
    "    # Optional: Define eval metric ops; here we add an accuracy metric.\n",
    "    is_correct = tf.equal(tf.argmax(input=targets, axis=1),\n",
    "                                                tf.argmax(input=output_activations, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "    eval_metric_ops = { \"accuracy\":  accuracy}\n",
    "\n",
    "    # 5. Return predictions/loss/train_op/eval_metric_ops in ModelFnOps object\n",
    "    return tf.contrib.learn.ModelFnOps(\n",
    "            mode=mode,\n",
    "            predictions=predictions_dict,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "custom_model = tf.contrib.learn.Estimator(model_fn=model_fn)\n",
    "\n",
    "# Train and evaluate the model.\n",
    "def evaluate_model(model, input_fn):\n",
    "    for i in range(6):\n",
    "        max_steps = train_data.shape[0]*n_epochs // batch_size\n",
    "        model.fit(input_fn=lambda: input_fn(train_data, train_labels), steps=max_steps)\n",
    "        print(model.evaluate(input_fn=lambda: input_fn(test_data, test_labels),\n",
    "                                                 steps=150))\n",
    "\n",
    "\n",
    "evaluate_model(custom_model, input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5bn1rtOnLPBP"
   },
   "source": [
    "# Custom model, simplified with tf.layers\n",
    "Instead of doing the matrix multiplications and everything ourselves, we can use tf.layers to simplify the definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Model parameters.\n",
    "batch_size = 32\n",
    "hidden1_units = 128\n",
    "learning_rate = 0.005\n",
    "input_dimension = train_data.shape[1]   # 784 = 28*28 pixels\n",
    "output_dimension = train_labels.shape[1]  # 6 classes\n",
    "\n",
    "def layers_custom_model_fn(features, targets, mode, params):\n",
    "    # 1. Configure the model via TensorFlow operations (using tf.layers). Note how\n",
    "    #    much simpler this is compared to defining the weight matrices and matrix\n",
    "    #    multiplications by hand.\n",
    "    hidden_layer = tf.layers.dense(inputs=features['images'], units=hidden1_units, activation=tf.nn.relu)\n",
    "    output_layer = tf.layers.dense(inputs=hidden_layer, units=output_dimension, activation=tf.nn.relu)\n",
    "    \n",
    "    # 2. Define the loss function for training/evaluation\n",
    "    loss = tf.losses.mean_squared_error(labels=targets, predictions=output_layer)\n",
    "    \n",
    "    # 3. Define the training operation/optimizer\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            learning_rate=learning_rate,\n",
    "            optimizer=\"SGD\")\n",
    "    \n",
    "    # 4. Generate predictions\n",
    "    predictions_dict = {\n",
    "        \"classes\":       tf.argmax(input=output_layer, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(output_layer, name=\"softmax_tensor\"), \n",
    "        \"logits\":        output_layer,\n",
    "    }\n",
    "    \n",
    "    # Define eval metric ops; we can also use a pre-defined function here.\n",
    "    accuracy = tf.metrics.accuracy(\n",
    "        labels=tf.argmax(input=targets, axis=1),\n",
    "        predictions=tf.argmax(input=output_layer, axis=1))\n",
    "    eval_metric_ops = {\"accuracy\":  accuracy}\n",
    "\n",
    "    # 5. Return predictions/loss/train_op/eval_metric_ops in ModelFnOps object\n",
    "    return tf.contrib.learn.ModelFnOps(\n",
    "        mode=mode,\n",
    "       predictions=predictions_dict,\n",
    "       loss=loss,\n",
    "       train_op=train_op,\n",
    "       eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "layers_custom_model = tf.contrib.learn.Estimator(\n",
    "        model_fn=layers_custom_model_fn)\n",
    "\n",
    "# Train and evaluate the model.\n",
    "evaluate_model(layers_custom_model, input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model using canned estimators\n",
    "Instead of defining our own DNN classifier, TensorFlow supplies a number of *canned* estimators that can save a lot of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Model parameters.\n",
    "hidden1_units = 128\n",
    "learning_rate = 0.005\n",
    "input_dimension = train_data.shape[1]   # 784 = 28*28 pixels\n",
    "output_dimension = train_labels.shape[1]  # 6 classes\n",
    "\n",
    "# Our model can be defined using just three simple lines...\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "images_column = tf.contrib.layers.real_valued_column(\"images\")\n",
    "# Task: Use the DNNClassifier Estimator to create the model in 1 line.\n",
    "canned_model = _\n",
    "\n",
    "# Potential exercises: play with model parameters, e.g. add dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to change the input_fn so that it returns integers representing the classes instead of one-hot vectors.\n",
    "def class_input_fn(data, labels):\n",
    "    input_images = tf.constant(\n",
    "        data, shape=data.shape, verify_shape=True, dtype=tf.float32)\n",
    "    # The next two lines are different.\n",
    "    class_labels = np.argmax(labels, axis=1)\n",
    "    input_labels = tf.constant(\n",
    "        class_labels, shape=class_labels.shape, verify_shape=True, dtype=tf.int32)\n",
    "    image, label = tf.train.slice_input_producer(\n",
    "        [input_images, input_labels], num_epochs=n_epochs)\n",
    "    dataset_dict = dict(images=image, labels=label)\n",
    "    batch_dict = tf.train.batch(\n",
    "        dataset_dict, batch_size, allow_smaller_final_batch=True)\n",
    "    batch_labels = batch_dict.pop('labels')\n",
    "    return batch_dict, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train and evaluate the model.\n",
    "evaluate_model(canned_model, class_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_dimension = train_data.shape[1]   # 784 = 28*28 pixels\n",
    "output_dimension = train_labels.shape[1]  # 6 classes\n",
    "batch_size = 32\n",
    "\n",
    "data_batch =  tf.placeholder(\"float\", shape=[None, input_dimension])\n",
    "label_batch = tf.placeholder(\"float\", shape=[None, output_dimension])\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Task: convert the batch_size x num_pixels (784) input to batch_size, height (28), width(28), channels\n",
    "# image_batch = # YOUR CODE HERE\n",
    "image_batch = tf.reshape(data_batch, [-1, 28, 28, 1])\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(image_batch, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 48])\n",
    "b_conv2 = bias_variable([48])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 48, 256])\n",
    "b_fc1 = bias_variable([256])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*48])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Task: add dropout to fully connected layer. Add a variable to turn dropout off in eval.\n",
    "\n",
    "W_fc2 = weight_variable([256, output_dimension])\n",
    "b_fc2 = bias_variable([output_dimension])\n",
    "\n",
    "output_activations = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "\n",
    "loss = tf.reduce_mean(\n",
    "           tf.nn.softmax_cross_entropy_with_logits(labels=label_batch, \n",
    "                                                   logits=output_activations))\n",
    "\n",
    "# Task: Switch from GradientDescentOptimizer to AdamOptimizer\n",
    "learning_rate = 0.01\n",
    "updates = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    random_indices = np.random.permutation(train_data.shape[0])\n",
    "    n_epochs = 5  # how often to go through the training data\n",
    "    max_steps = train_data.shape[0]*n_epochs // batch_size\n",
    "    for i in range(max_steps):\n",
    "        batch_start_idx = (i % (train_data.shape[0] // batch_size)) * batch_size\n",
    "        batch_indices = random_indices[batch_start_idx:batch_start_idx+batch_size]\n",
    "        batch_loss, _ = sess.run(\n",
    "            [loss, updates], \n",
    "            feed_dict = {\n",
    "                data_batch : train_data[batch_indices,:],\n",
    "                label_batch : train_labels[batch_indices,:]\n",
    "            })\n",
    "        if i % 100 == 0 or i == max_steps - 1:\n",
    "            random_indices = np.random.permutation(train_data.shape[0])\n",
    "            print(\"Batch-Loss at iteration {}/{}: {}\".format(i, max_steps-1, batch_loss))\n",
    "    \n",
    "            test_predictions = sess.run(\n",
    "                output_activations,\n",
    "                feed_dict = {\n",
    "                    data_batch : test_data,\n",
    "                    label_batch : test_labels\n",
    "                })\n",
    "            wins = np.argmax(test_predictions, axis=1) == np.argmax(test_labels, axis=1)\n",
    "            print(\"Accuracy on test: {}%\".format(100*np.mean(wins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "02_Quickdraw_FFNN.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [conda env:cs224n]",
   "language": "python",
   "name": "conda-env-cs224n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
